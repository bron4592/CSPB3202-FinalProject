{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brandon O'Neill<br>\n",
    "CSPB 3202<br>\n",
    "August 10, 2020<br>\n",
    "Final Project<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "For my project, I decided to try and build an RL model for the mountain car environment. This was done using Python 3, Ptan, and gym. The mountain car problem is one where a car must accelerate to the left or right in order to gain enough momentum to get to the top of the hill. Below you will see an example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Problem\n",
    "Here you can see the environment with a fully random policy. The car is unable to make any progress up the hill at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"car.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"car.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "  env.render()\n",
    "  action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "  observation, reward, done, info = env.step(action)\n",
    "\n",
    "  if done:\n",
    "    observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mountain Climbing Probem   \n",
    "   \n",
    "   ### Environment:\n",
    "    Actions:\n",
    "         Accelerate to the Left\n",
    "         Don't accelerate\n",
    "         Accelerate to the Right\n",
    "    Reward:\n",
    "         Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
    "         on top of the mountain.\n",
    "         Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "    Starting State:\n",
    "         The position of the car is assigned a uniform random value in\n",
    "         [-0.6 , -0.4].\n",
    "         The starting velocity of the car is always assigned to 0.\n",
    "    Episode Termination:\n",
    "         The car position is more than 0.5\n",
    "         Episode length is greater than 200"
   ]
  },
  {
   "attachments": {
    "car.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAGRCAIAAADQIpEuAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABgHSURBVHhe7d3ZehtXmiXQ82pVXd1dTqcnSZREkZKtefIg23IOfZUP0pd91c9S7+QK/dgQQJEQQRJATGt98VUe7wgGQeCcs0sSSbT/+//+//0Xv9x/3h0/39tw1NlfPlxWx/HLd8evfn1QRzf48J91fLxgm2N5k98evP7tpDve/H765v3p2xwPHfM+Ps6EbmJ006ObJN1ku/+iO97de/bu7tNf7j7t/u+7u89+vfvst+64tzienz0qrAt+PXr67ujJL3ced8fP3XF7eXTJ0ZN3R09zn9zqk/vs5Phw29+74/6L7nh/vxvXA+u+inpgP9/+4adbj3689cNPi4f04ZF0Fz9/3118/PKP+y+64/2HOywf592n3fHhYdc9u8sWF9SX/PTX1T2//3F1dP9ZR5dfcOQJ+fCcbD4+3HNxrG7bPewLj+UnXdy8+/DuUXUvxLmH/Uf3BX78GvPMdNd0X8WHJ2dx1Md2x+IxPF57AI/efrd2nPnU60ce1YVXfnx4mQn3Fg8sD6ke1YvutfvwtHcX1HObp2J587V7VrJ4ipbP5/px7klYPg+LJ+H41d9yvOyOxdPSna2XfjGLVhNg8RR9fJbWHtX6q5Nj/VVYfpk1IZfP9uIOmYf1tSyOj8/S2WORX/x81pd/7nMtvpDuWa2va/EFrn2Nmdvrr3s9V8tj03N+/rOfORb5hitv//DhQd558sufZ7WunzIEgNlr3f8zniEAzF47efN7hgAwe+1ULwLAUjt9+z5DAJg9vQgAK3oRAFZ83w0ArLQHr3/NEABmrx2/+PRH/QFgttq9Zz9mCACz1+788DpDAJi99t3D5xkCwOy1r48fZwgAs9e+vPsoQwCYvfaXo9MMAWD22hd6EQCW/D0qAMPVWvuv//pw5L/3r3113/fdADBQH3vxkyOn96B9e/osQwAYmE29+MmRq3eh3fZz/QAM1Za9+MmRD76WdtfvgQNgqK7Xi58cudd22vFLvzccgIHqoRdPXv+WIQAMTA9/j+p9iQEYrB6+7+b07fsMAWBgNvViTu9Be/j2jwwBYGA+9mL+e//0IgDD1fViRoeiFwEYLr0IACt6EQBW9CIArOhFAFjRiwCwohcBYEUvAsCKXgSAFb0IACt6EQBW9CIArOhFAFjRiwCwohcBYEUvAsBKL734PkMAGJgeevFULwIwVH304pvfMwSAgemhF09e/5YhAAxMD714/PJdhgAwMD304r1nP2UIAAPTQy/eefw6QwAYmB568duHzzMEgIHpoRe/On6cIQAMTA+9+JejhxkCwMD00Iv/eftBhgAwMD304v++dZwhAAxMH39evOXPiwAMVA+9+MXRaYYAMDA99OJf73+fIQAMTA+9+M3J0wwBYGB66MVb37/KEAAGpodePHr6Y4YAMDA99OL9F79kCAAD00MvPnj1a4YAMDA99KL3JQZgsHroxdM37zMEgGHo6vDmcq8raqdv9SIAfUqP7V8+32d1f178PUMAOJQ0VX/yOM7x74sAHE5KaUjyyJba8ct3GQLAHqR/riIfeTO512Vy9VK79/ynDAFgp9I8l8nV+5fPd1bOLbWjx28yBIBdSOF8Vi7tTx7H+V689ehFhgBwM6mazXLdgLVvTp5kCADXktLbIBeNhPdfBOD6Un0b5KJRaV/efZQhAGwt1XeRXDFOehGAq0n7XSRXjJm/RwVgW2m/c3J6EnzfDQCXSwGek9MT4uc0ALhEOvCsnJucdvTEz/UDcLF04Fk5N1F+DxwAF0gHnpVzk+b3hgPwqdTgmpyYAe8zBcBKavCsnJsH70sMQKQG1+TEnLTTt+8zBGCuUoNrcmJ+9CLA3KUJ1+TELLUTf48KMFepwTU5MWPtge+7AZilNOGanJi3dv/FLxkCMBtpwqWkdM/M3ac/ZgjADKQJ1+QEpd36/lWGAExdmnApKWvaNydPMwRg0lKGS0k5q/31nvdfBJi+lOFSUs5pXxydZgjAFKUJl5KyQfvi9kmGAExOynApKZu1L+748yLANKUMl5LyWe3Lu48yBGBCUoZLSblM++r4SYYATEKacCkp22nfPnyRIQDjlzJcSsrW2u0fXmcIwMilDJeSchXt6InfAwcwBSnDpaRcUbv79KcMARitlGFJxLW0o6c/ZwjAOKUPSyKuqx098edFgBFLH5ZE3EC789i/LwKMVfqwJOJm2q3v9SLAKKUPSyJurH336E2GAIxEynApKbvQvjn184sAY5IyXErKjrSvHni/foDRSBkuJWV32pf3/R44gHFIGS4lZafaF0fPMgRg2NKHGnGf2v+6/TRDAIZNKR5A+4/vHmcIwCAt6nAhEXvT/v2bHzIEYHjShyUR+9T+7evvMwRgYNKHJRF71v5dLwIMUvqwJGL/2v/w96gAw5M+LIk4iPY/b/m+G4BhSR+WRBxK+887fk4DYEDShyURB9S+vOf33QAMRfqwJOKw2tcnfm84wCCkD0siDq5998j7LwL0L31YEtGHdufxLxkC0JP0YUlET9rdZ79lCEBPUoklET1p91/8kSEAfUgflkT0pz149bcMATi49GFJRK/ag9d/zxCAw0oflkT0zZ8XAfqRPiyJGAD/vgjQg/RhScQw+H5UgENLH5ZEDEa7/fjnDAHYv/RhScSQtG9P/R44gANJH5ZEDEz76v7zDAHYp/RhScTwtL/cfZIhAHuTPiyJGKT2xR3v1w+wd6nEkohBal8c6UWA/UoflkQMVfvS36MC7FP6sCRiwNrXx96vH2Bf0oclEcPWvnv4JkMAdip9WBIxeN6XGGAv0oclEWPQ7j3/PUMAdiR9WBIxEu34pffTANil9GFJxHi0k9f/yBCAXUgllkSMRzt9888MAbix9GFJxKi007f/J0MAbiZ9WBIxNu3hj3oRYAfShyURI6QXAXYgfVgSMU56EeCm0oclEaOlFwFuKpVYEjFaH3rRawlwbYstdCERY7bqxU4yALaT3bMkYuTO9GInMQCXyb5ZEjF++ffFvLBlcQKAz8iOWRIxCe1h/Vx/XtuyOAHAJtkuSyKmIr34YbRmkQBwoeyVJRFTceb3wOVFLokAOCu7ZEnEhHz6+1HzUpdEACxlfyyJmJYLfm94XvCSCADb4zzoRYCtZFssiZiii99nKq98SQQwb9kTSyKm6OJe7OTFL4kA5iq7YUnERG3sxU6mQEkEMD/ZB0sipmv184sXykQoiQDmJDtgScSk6UWAjbL9lURM3eXvv5gZURIBzEP2vpKIqbu8FzuZFCURwNRl1yuJmIGterGTqVESAUxX9ruSiHnYthc7mSAlEcAUZacriZgNvQjwqex0JRGzcYVe7GSalEQA05I9riRiTq7Wi51MlpIIYCqyu5VEzMyVe7GTKVMSAYxf9rWSiPnRiwAfZFMriZil6/RiJ3OnJAIYs+xoJRGzdM1e7GT6lEQA45S9rCRirq7fi51MopIIYGyyi5VEzNiNerGTqVQSAYxH9q+SiHnTi8CsZf8qiZi3m/ZiJxOqJAIYg+xcJRGzt4Ne7GRalUQAw5Y9qySCXfViJ5OrJAIYquxWJREUvQjMUXarkgjKznqxkylWEgEMT/apkgiWdtmLnUy0kghgSLJDlUSwZse92Ml0K4kAhiF7U0kEZ+lFYEayN5VEcNbue7GTSVcSAfQtu1JJBOfspRc7mXolEUB/sh+VRHCRffViJxOwJALoQ3aikgg20IvAxGUbKolgsz32YiczsSQCOKzsQSURbLbfXuxkMpZEAIeS3ackgs/aey92MiVLIoD9y75TEsFl9CIwTdl0SiLYwiF6sZO5WRIB7FN2nJIItnCgXuxkepZEAPuRvaYkgu0crhc7maQlEcCuZZcpiWBrehGYlGwxJRFcxUF7sZPZWhIB7E72l5IIruLQvdjJhC2JAHYhO0tJBFfUQy92Mm1LIoCbyZ5SEsHV6UVgIrKnlERwdf30YieTtyQCuK7sJiURXEtvvdjJFC6JAK4u+0hJBNfVZy92MpFLIoCryA5SEsEN6EVg3LKDlERwAz33YifTuSQC2E72jpIIbqb/XuxkUpdEAJfJrlESwY0Nohc7mdolEcBm2S9KItgFvQiMUvaLkgh2YSi92MkEL4kALpKdoiSCHRlQL3YyzUsigLOyR5REsDvD6sVOJntJBLCU3aEkgp3Si8CYZHcoiWCnBteLnUz5kgjA5sBBDLEXO5n4JREwb9kRSiLYg4H2YifTvyQC5ip7QUkE+6EXgRHIXlASwX4Mtxc7WQQlETA/2QVKItibQfdiJ0uhJALmJOu/JIJ9GnovdrIgSiJgHrLySyLYM70IDFdWfkkEezaCXuxkWZREwNRlzZdEsH/j6MVOFkdJBExXVntJBAehF4HByVIvieBQRtOLnaySkgiYoqzzkggOZUy92MlCKYmAackKL4nggEbWi50sl5IImIqs7ZIIDksvAkORhV0SwcGNrxc7WTclETB+WdUlERzcKHuxk6VTEgFjlvVcEkEfxtqLnSygkggYp6zkkgh6oheB/mUll0TQkxH3YifLqCQCxiZruCSC/oy7FztZTCURMB5ZvSUR9Gr0vdjJkiqJgDHIui2JoG96EehN1m1JBH2bQi92srBKImDYsmJLIhiAifRiJ8urJAKGKmu1JIJhmE4vdrLISiJgeLJKSyIYDL0IHFpWaUkEgzGpXuxkqZVEwJBkfZZEMCRT68VOFlxJBAxDVmZJBAMzwV7sZNmVREDfsiZLIhgevQgcQhZkSQSDNM1e7GT9lURAf7IaSyIYpMn2YidLsCQC+pB1WBLBUE25FztZiCURcFhZgSURDJheBPYoy68kgmGbeC92siJLIuBQsvZKIhi26fdiJ4uyJAL2L6uuJILBm0UvdrI0SyJgn7LeSiIYg7n0YicLtCQC9iMrrSSCkdCLwO5lpZVEMBIz6sVOlmlJBOxa1lhJBOMxr17sZLGWRMDuZHWVRDAqs+vFTpZsSQTsQtZVSQRjoxeBncm6KolgbObYi50s3JIIuJmsqJIIRmimvdjJ8i2JgOvKWiqJYJzm24udLOKSCLi6rKKSCEZLL0Yi4IqyhEoiGLNZ92Inq7kkAq4i66ckgjGbey92sqBLImA7WTklEYycXvwgy7okAi6TNVMSwfjpxcjiLomAzbJaSiKYBL0YWd8lEbBZVktJBJOgF1eyxEsi4CJZJyURTIVePCMLvSQCzsoKKYlgQvTip7LcSyJgKWujJIJp0YsXyKIviQBLg3nQixfIui+JAEuDedCLF8vSL4lgeA45RRefayERTJFe3CgbQEkEQ5LZWRLtTT5NSQQTpRc/J9tASQTDkHm5Jif2IJ+gJILp0ouXyGZQEkHfMiPPyemdyq1LIpg0vXiJ7AclEfQq03GzXLcjuWlJBJOmFy+XLaEkgp5kIl4mV99YblcSwdS1h2/14uWyMZREcHCZgtvJx9xAblQSwQy0U724nWwPJREcUCbfVeQjryW3KIlgHrpe/GeGXCabREkEB5Fpd3X5+CvKB5dEMBvt5I1e3Fb2iZII9i9z7rpyl63lw0oimJN28vofGbKF7BYlEexTZtvN5F7byceURDAn7cGrv2fIdrJhlESwH5lnu5A7XiZXl0QwM+345R8ZsrVsGyUR7Fpm2LX861//ymhN7rtZriuJYH7a/Rd68TqyeZREsDuZW1fU1eF5OXfZRM1FJRHMUteL7zPkirKFlESwC5lVV5EO3Cy33iB3KYlgrtq9579nyBVlFymJ4MYypa4i1XeZfIJzcpeSCGas68XfMuTqspeURHADmUxXlN67TD7HOblLSQQz1u4+04s3ku2kJIJryTS6opTedvKZ1uQuJRHMWzt6+muGXFc2lZIIrigT6OrSeNvJJ1vKLUoimL129OSXDLmBbC0lEWwtU+da0njbyecr+fiSCOiWxu0ffsqQm8kGUxLBFjJpriuNt518StMVNmu3Hr3NkBvLNlMSwWdlutxAGm875z/pIgE+at+cvs6QG8tOUxLBZpkrN5PG2875z7tIgI/a1w9eZMguZLMpiWCDTJSbSeNt55NPungYwLr21/vPMmRHsuWURHBOpsgupPQu88knXTwM4BPty3tPM2R3svGURLAmk2NH0nuXydUljwM4p315Vy/uRbafkghKpsVOpfo2y3UljwO4iD8v7lE2oZKI2cuEuIo021k5V3LrP//MubO6PNeVxZXAJv59cb+yFZVEzFtmw9ZSbhdZXJD7bra4bCERsFn76/HzDNmPbEglEfOW2XCZtN9n5Y6b5V4lEfBZ7euTlxmyN9mWSiLmLbPhs1J9l8kdL5IblUTAZfxc/yFkZyqJmL1MiA1SetvJHc/KjUoiYAvtO78H7iCyP5VEzF4mxEXSeNvJ7c7KjUoiYAvt1vc/ZsieZYsqiZi9TIhz0njbyb3W5C4lEbCddvvxzxmyf9moSiJmLxPirDTednKjpdyiJAK21u54/8XDynZVEjF7mRBr0njbyV1KPr4kAq6iHT19lyGHkk2rJGL2MiGW0njbyS1MLdiFdvfZbxlyQNm6SiJmLxOipPG2c/7DFwlwDXqxN9nASiJmLxPi6r2YDyuLWwHX0+4+1Yu9yTZWEjF7mRBbV+P6h3QWNwGurR090Yt9ymZWEjF7mRBbVOP6xZ3FhwM30W794PtuepYtrSRi9j5OhhTgRRaXmTawW+3b058ypD+L3W0hEZyVMlx+l00nM8acgZ1qXx37PXCDkB2uJILNMldKImAX2pf3/N7wocgmVxLBRTJLSiJgR9pfjl5lyDBkt7PfsUHmR0kE7I5eHJxseCURLGVmlETATvl71CHKtlcSgYkBB9G+fuB9poYom19JxLxlNpREwB607x55P42ByhZYEjFXmQclEbAf7c5jv+9muLIRlkTMT2ZASQTsTbv37I8MGaRshyURc5LXviQC9qkdv/x7hgxVNsWSiHnIq14SAXvWTl7/I0MGLFtjScSk5cVeSgrsXzt9888MGbZskEtJmaK8xktJgYNop2/14mhkm1xKyrTk1V1KChxKe6gXRyWb5VJSpiKv61JS4ID8eXF8smUuJWX88oqWRMDBtdM3vu9mlLJ9lkSMWV7LkgjoQ9eLfk5jrLKJlkSMU17FkgjoSTt5/bcMGaFspUtJGY+8cktJgf50vej33YxbNtSlpIxBXrOlpECv9OIUZFtdSsqw5dVaSgr0TS9ORDbXpaQMVV6npaTAAOjFSckuu5SUIclrs5QUGAy9ODXZbpeSMgx5VZaSAkPS9aLvR52abLpLSelbXo+lpMDA+PnFacrWuyYn6ENegzU5AQyP33czZdmDl5JyWHn2l5ICQ9VO3+rFKctmvJSUQ8nzvpQUGDC/N3z6siWvyQn2Kc/1mpwAhk0vzkX25qWk7Eee5aWkwBjoxRnJJr0mJ9idPLNrcgIYiXb6Ri/OSLbqNTnBLuQ5XZMTwHi0k9e+72Z2smevyQmuK8/jmpwAxqbrRT+/OEfZvNfkBFeUp++snANGqD145ffdzFd28TU5wXbyrK3JCWC02vFLvx911rKdn5VzbJZn6qycA8as3X+hF7HLX0GenbNyDhi/rhffZ8i8ZYM/K+coeVLOyjlgKvQiZ2SzPyvn5i3PxVk5B0xIO/b3qJyTXf+snJuffP1n5RwwOb7vho3SAOfk9NTlqz0np4GJ8nMafE6q4CK5YoryFZ6T08Ck+X03XC61cJFcMRX5qi6SK4Cpayfel5jtpB82yEXjlK9hg1wEzIPfG86VpS4ukivGI4/7IrkCmBm9yDWlPTbIRUOVR7lBLgJmyd+jciNpks/KpX3Lo/msXArMmPfTYDdSLJfJ1YeSz3qZXA3Q7Rt+fpEdSs9sLR+2O7nv1vJhAEvt3vPfM4TdSe3cQG60QS66rtwF4Jx29ORdhrAf6aK+5dEAfFa79f1PGcL+paMOJZ8VYGvt24dvMoSDS33tTu4LcF3tm9PXGcKQpOg2yEUAu9a+OXmVIQDMnl4EgBV/jwoAK+27R28zBIDZa7d/+DlDAJi9dvT01wwBYPba/RfvMwSA2WsPXv0tQwCYPe8zBQArXS96X2IAiPbAnxcBYMn7EgPAivclBoAV70sMACvelxgAVrwvMQCseD8NAFjx/osAsKIXAWDF36MCwIr3JQaAFe9LDAAr3pcYAFa8LzEArHhfYgBY8f6LALCiFwFgxfsSA8CK9yUGgJV299lvGQLA7LX8LwDw55//Dad2pZ5CGi51AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![car.png](attachment:car.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "I decided to use Deep Q Learning to solve this problem. QLearning is a model free reinforcement learning technique that can be used to find the optimal action selection policy using Q function without requiring a model of the environment. Q-learning eventually finds an optimal policy. I used a number of variables sourced from other QLearning models and implemented the structure that iterates 5000 times. At the completion of these iterations, the optimal policy will be returned. This optimal policy can then be used in the gym environment to run the car up the mountain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 -- Total Reward : -210.\n",
      "Iteration No: 51 -- Total Reward : -212.\n",
      "Iteration No: 101 -- Total Reward : -208.\n",
      "Iteration No: 151 -- Total Reward : -207.\n",
      "Iteration No: 201 -- Total Reward : -206.\n",
      "Iteration No: 251 -- Total Reward : -213.\n",
      "Iteration No: 301 -- Total Reward : -1.\n",
      "Iteration No: 351 -- Total Reward : -210.\n",
      "Iteration No: 401 -- Total Reward : -213.\n",
      "Iteration No: 451 -- Total Reward : -210.\n",
      "Iteration No: 501 -- Total Reward : -211.\n",
      "Iteration No: 551 -- Total Reward : -213.\n",
      "Iteration No: 601 -- Total Reward : -208.\n",
      "Iteration No: 651 -- Total Reward : -209.\n",
      "Iteration No: 701 -- Total Reward : -215.\n",
      "Iteration No: 751 -- Total Reward : -211.\n",
      "Iteration No: 801 -- Total Reward : -210.\n",
      "Iteration No: 851 -- Total Reward : -205.\n",
      "Iteration No: 901 -- Total Reward : -1.\n",
      "Iteration No: 951 -- Total Reward : -207.\n",
      "Iteration No: 1001 -- Total Reward : -218.\n",
      "Iteration No: 1051 -- Total Reward : -212.\n",
      "Iteration No: 1101 -- Total Reward : -209.\n",
      "Iteration No: 1151 -- Total Reward : -212.\n",
      "Iteration No: 1201 -- Total Reward : -210.\n",
      "Iteration No: 1251 -- Total Reward : -213.\n",
      "Iteration No: 1301 -- Total Reward : -211.\n",
      "Iteration No: 1351 -- Total Reward : -205.\n",
      "Iteration No: 1401 -- Total Reward : -214.\n",
      "Iteration No: 1451 -- Total Reward : -206.\n",
      "Iteration No: 1501 -- Total Reward : -207.\n",
      "Iteration No: 1551 -- Total Reward : -217.\n",
      "Iteration No: 1601 -- Total Reward : -205.\n",
      "Iteration No: 1651 -- Total Reward : -214.\n",
      "Iteration No: 1701 -- Total Reward : -213.\n",
      "Iteration No: 1751 -- Total Reward : -214.\n",
      "Iteration No: 1801 -- Total Reward : -212.\n",
      "Iteration No: 1851 -- Total Reward : -210.\n",
      "Iteration No: 1901 -- Total Reward : -216.\n",
      "Iteration No: 1951 -- Total Reward : -211.\n",
      "Iteration No: 2001 -- Total Reward : -210.\n",
      "Iteration No: 2051 -- Total Reward : -207.\n",
      "Iteration No: 2101 -- Total Reward : -214.\n",
      "Iteration No: 2151 -- Total Reward : -210.\n",
      "Iteration No: 2201 -- Total Reward : -211.\n",
      "Iteration No: 2251 -- Total Reward : -208.\n",
      "Iteration No: 2301 -- Total Reward : -212.\n",
      "Iteration No: 2351 -- Total Reward : -213.\n",
      "Iteration No: 2401 -- Total Reward : -210.\n",
      "Iteration No: 2451 -- Total Reward : -210.\n",
      "Iteration No: 2501 -- Total Reward : -210.\n",
      "Iteration No: 2551 -- Total Reward : -214.\n",
      "Iteration No: 2601 -- Total Reward : -205.\n",
      "Iteration No: 2651 -- Total Reward : -211.\n",
      "Iteration No: 2701 -- Total Reward : -210.\n",
      "Iteration No: 2751 -- Total Reward : -208.\n",
      "Iteration No: 2801 -- Total Reward : -216.\n",
      "Iteration No: 2851 -- Total Reward : -214.\n",
      "Iteration No: 2901 -- Total Reward : -210.\n",
      "Iteration No: 2951 -- Total Reward : -210.\n",
      "Iteration No: 3001 -- Total Reward : -1.\n",
      "Iteration No: 3051 -- Total Reward : -207.\n",
      "Iteration No: 3101 -- Total Reward : -209.\n",
      "Iteration No: 3151 -- Total Reward : -215.\n",
      "Iteration No: 3201 -- Total Reward : -207.\n",
      "Iteration No: 3251 -- Total Reward : -176.\n",
      "Iteration No: 3301 -- Total Reward : -209.\n",
      "Iteration No: 3351 -- Total Reward : -212.\n",
      "Iteration No: 3401 -- Total Reward : -210.\n",
      "Iteration No: 3451 -- Total Reward : -205.\n",
      "Iteration No: 3501 -- Total Reward : -209.\n",
      "Iteration No: 3551 -- Total Reward : -212.\n",
      "Iteration No: 3601 -- Total Reward : -209.\n",
      "Iteration No: 3651 -- Total Reward : -207.\n",
      "Iteration No: 3701 -- Total Reward : -207.\n",
      "Iteration No: 3751 -- Total Reward : -207.\n",
      "Iteration No: 3801 -- Total Reward : -209.\n",
      "Iteration No: 3851 -- Total Reward : -209.\n",
      "Iteration No: 3901 -- Total Reward : -206.\n",
      "Iteration No: 3951 -- Total Reward : -208.\n",
      "Iteration No: 4001 -- Total Reward : -208.\n",
      "Iteration No: 4051 -- Total Reward : -208.\n",
      "Iteration No: 4101 -- Total Reward : -211.\n",
      "Iteration No: 4151 -- Total Reward : -208.\n",
      "Iteration No: 4201 -- Total Reward : -207.\n",
      "Iteration No: 4251 -- Total Reward : -1.\n",
      "Iteration No: 4301 -- Total Reward : -209.\n",
      "Iteration No: 4351 -- Total Reward : -205.\n",
      "Iteration No: 4401 -- Total Reward : -213.\n",
      "Iteration No: 4451 -- Total Reward : -210.\n",
      "Iteration No: 4501 -- Total Reward : -207.\n",
      "Iteration No: 4551 -- Total Reward : -210.\n",
      "Iteration No: 4601 -- Total Reward : -179.\n",
      "Iteration No: 4651 -- Total Reward : -205.\n",
      "Iteration No: 4701 -- Total Reward : -211.\n",
      "Iteration No: 4751 -- Total Reward : -212.\n",
      "Iteration No: 4801 -- Total Reward : -207.\n",
      "Iteration No: 4851 -- Total Reward : -206.\n",
      "Iteration No: 4901 -- Total Reward : -211.\n",
      "Iteration No: 4951 -- Total Reward : -1.\n",
      "Mean score :  -2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totStates = 40 \n",
    "totIterations = 5000\n",
    "learningRate = 1.0\n",
    "minLearningRate = 0.005\n",
    "maxStep = 10000\n",
    "\n",
    "# parameters for q learning\n",
    "epsilon = 0.05\n",
    "gamma = 1.0\n",
    "\n",
    "def simulation(environment, policy, render=False):\n",
    "    observation= environment.reset()\n",
    "    reward = 0\n",
    "    count = 0\n",
    "    for _ in range(maxStep):\n",
    "        p,v = obs(environment, observation)\n",
    "        action = policy[p][v]\n",
    "        if render:\n",
    "            environment.render()\n",
    "        observation, reward, done, _ = environment.step(action)\n",
    "        reward += gamma ** count * reward\n",
    "        count += 1\n",
    "        if done:\n",
    "            break\n",
    "    return reward\n",
    "\n",
    "\n",
    "def obs(environment, observation):\n",
    "    low = environment.observation_space.low\n",
    "    high = environment.observation_space.high\n",
    "    dx = (high - low) / totStates\n",
    "    p = int((observation[0] - low[0])/dx[0])\n",
    "    v = int((observation[1] - low[1])/dx[1])\n",
    "    \n",
    "    return p, v\n",
    "\n",
    "\n",
    "environment = gym.make('MountainCar-v0')\n",
    "environment.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "qTable = np.zeros((totStates, totStates, 3))\n",
    "\n",
    "for i in range(totIterations):\n",
    "    observation = environment.reset()\n",
    "    totReward = 0\n",
    "    eta = max(minLearningRate, learningRate * (0.85 ** (i//100)))\n",
    "\n",
    "    for j in range(maxStep):\n",
    "        p, v = obs(environment, observation)\n",
    "\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.choice(environment.action_space.n)\n",
    "        else:\n",
    "            logits = qTable[p][v]\n",
    "            logits_exp = np.exp(logits)\n",
    "            probabilities = logits_exp / np.sum(logits_exp)\n",
    "            action = np.random.choice(environment.action_space.n, p=probabilities)\n",
    "            observation, reward, done, _ = environment.step(action)\n",
    "\n",
    "        totReward += reward\n",
    "        p_, v_ = obs(environment, observation)\n",
    "        qTable[p][v][action] = qTable[p][v][action] + eta * (reward + gamma *  np.max(qTable[p_][v_]) - qTable[p][v][action])\n",
    "        if done:\n",
    "            break\n",
    "    if i % 50 == 0:\n",
    "        print('Iteration No: %d -- Total Reward : %d.' %(i+1, totReward))\n",
    "\n",
    "solution_policy = np.argmax(qTable, axis=2)\n",
    "solution_policy_scores = [simulation(environment, solution_policy, False) for _ in range(100)]\n",
    "print(\"Mean score : \", np.mean(solution_policy_scores))\n",
    "simulation(environment, solution_policy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Demonstration of Solution Policy Solving the Problem\n",
    "As seen in the simulation below, the car is now able to successfully climb the hill in an efficient manner. With the optimal policy now on display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-118.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_simulation(environment, solution_policy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Conclusion\n",
    "The QLearning worked perfectly to solve the gym environment. The car is able to successfully climb up the hill. I learned a lot in this project about how to develop a policy and intergrate it with the already built environments using gym. This course has given me the tools to successfully develop little AI projects. In the future, I would love to implement more complex projects using less already built applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://gym.openai.com/envs/MountainCar-v0/ <br>\n",
    "https://github.com/openai/baselines/<br>\n",
    "https://github.com/Shmuma/ptan/blob/master/docs/intro.ipynb<br>\n",
    "https://github.com/omerbsezer/Qlearning_MountainCar<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
